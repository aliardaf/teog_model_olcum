model,teog 2013 puanÄ±
GPT-3.5-turbo-0125,392.0
GPT-4o-mini,403.0
"Model(
  (model): LlamaModel(
    (embed_tokens): QuantizedEmbedding(49152, 576, group_size=64, bits=4)
    (layers.0): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.1): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.2): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.3): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.4): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.5): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.6): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.7): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.8): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.9): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.10): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.11): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.12): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.13): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.14): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.15): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.16): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.17): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.18): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.19): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.20): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.21): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.22): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.23): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.24): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.25): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.26): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.27): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.28): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.29): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (norm): RMSNorm(576, eps=1e-05)
  )
)",380.84964
"Model(
  (model): LlamaModel(
    (embed_tokens): QuantizedEmbedding(49152, 576, group_size=64, bits=4)
    (layers.0): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.1): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.2): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.3): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.4): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.5): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.6): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.7): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.8): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.9): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.10): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.11): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.12): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.13): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.14): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.15): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.16): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.17): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.18): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.19): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.20): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.21): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.22): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.23): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.24): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.25): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.26): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.27): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.28): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.29): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (norm): RMSNorm(576, eps=1e-05)
  )
)",380.84964
"Model(
  (model): LlamaModel(
    (embed_tokens): QuantizedEmbedding(49152, 576, group_size=64, bits=4)
    (layers.0): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.1): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.2): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.3): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.4): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.5): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.6): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.7): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.8): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.9): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.10): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.11): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.12): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.13): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.14): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.15): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.16): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.17): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.18): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.19): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.20): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.21): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.22): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.23): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.24): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.25): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.26): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.27): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.28): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (layers.29): TransformerBlock(
      (self_attn): Attention(
        (q_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (k_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (v_proj): QuantizedLinear(input_dims=576, output_dims=192, bias=False,group_size=64, bits=4)
        (o_proj): QuantizedLinear(input_dims=576, output_dims=576, bias=False,group_size=64, bits=4)
        (rope): DynamicNTKScalingRoPE()
      )
      (mlp): MLP(
        (gate_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
        (down_proj): QuantizedLinear(input_dims=1536, output_dims=576, bias=False,group_size=64, bits=4)
        (up_proj): QuantizedLinear(input_dims=576, output_dims=1536, bias=False,group_size=64, bits=4)
      )
      (input_layernorm): RMSNorm(576, eps=1e-05)
      (post_attention_layernorm): RMSNorm(576, eps=1e-05)
    )
    (norm): RMSNorm(576, eps=1e-05)
  )
)",380.84964
mlx-community/SmolLM-135M-Instruct-4bit,380.84964
mlx-community/SmolLM-135M-Instruct-4bit,380.84964
-community/SmolLM-135M-Instruct-4bit,380.84964
-community/SmolLM-135M-Instruct-4bit,380.84964
-community/SmolLM-135M-Instruct-4bit,0.0
-community/SmolLM-135M-Instruct-4bit,120.0
